[
    {
        "content": "<p>On behalf of the <a href=\"https://www.vision4ai.eu/\">VISION Project</a>, I would like to invite you to a unique series of AI lectures, that are being regularly prepared in cooperation with the main European AI initiatives and four newly emerging European networks of AI excellence centres (ICT48).</p>\n<p><strong>Lecture by Prof. Pietro Perona: Measuring algorithmic bias in face analysis ‚Äî towards an experimental approach</strong></p>\n<p><span aria-label=\"calendar\" class=\"emoji emoji-1f4c5\" role=\"img\" title=\"calendar\">:calendar:</span> Tuesday <strong>Feb 23rd, 2021 17:00 ‚Äì 18:00 CET</strong><br>\n<span aria-label=\"point right\" class=\"emoji emoji-1f449\" role=\"img\" title=\"point right\">:point_right:</span> Please use the following zoom link to attend the lecture: <a href=\"https://authgr.zoom.us/j/98481711168\">https://authgr.zoom.us/j/98481711168</a><br>\n<span aria-label=\"light bulb\" class=\"emoji emoji-1f4a1\" role=\"img\" title=\"light bulb\">:light_bulb:</span> Learn more about the lecture: <a href=\"http://www.i-aida.org/future-lectures/\">http://www.i-aida.org/future-lectures/</a></p>\n<p>The lecture is FREE!</p>\n<p>Abstract: Measuring algorithmic bias is crucial both to assess algorithmic fairness, and to guide the improvement of algorithms. Current methods to measure algorithmic bias in computer vision, which are based on observational datasets, are inadequate for this task because they conflate algorithmic bias with dataset bias. To address this problem I will propose experimental method for measuring algorithmic bias of face analysis algorithms, which manipulates directly the attributes of interest, e.g., gender and skin tone, in order to reveal causal links between attribute variation and performance change. The method is based on generating synthetic ‚Äútransects‚Äù of matched sample images that are designed to differ along specific attributes while leaving other attributes constant. A crucial aspect of our approach is relying on the perception of human observers, both to guide manipulations, and to measure algorithmic bias. Besides allowing the measurement of algorithmic bias, synthetic transects have other advantages with respect to observational datasets: sampling  attributes more evenly, allowing for more straightforward bias analysis on minority and intersectional groups, enabling prediction of bias in new scenarios, reducing ethical and legal challenges, and they are economical and fast to obtain, helping make bias testing affordable and widely available. The method is validated by comparing it to a study that employs the traditional observational method for analyzing bias in gender classification algorithms. The two methods reach different conclusions. While the observational method reports gender and skin color biases, the experimental method reveals biases due to gender, hair length, age, and facial hair.</p>",
        "id": 225805545,
        "sender_full_name": "Anna Tahovsk√° (CLAIRE Office, CZ)",
        "timestamp": 1612945360
    },
    {
        "content": "<p>On behalf of <a href=\"https://www.vision4ai.eu/\">VISION Project</a>, I would like to invite you to one of the upcoming joint ICT48 activities under the AIDA AI Excellence Lecture Series:<br>\n<strong>Lecture by Prof. Bj√∂rn Schuller: There will be Artificial Emotional Intelligence</strong></p>\n<p><span aria-label=\"calendar\" class=\"emoji emoji-1f4c5\" role=\"img\" title=\"calendar\">:calendar:</span> Tuesday <strong>Mar 9th, 2021, 17:00 ‚Äì 18:00 CET</strong><br>\n<span aria-label=\"point right\" class=\"emoji emoji-1f449\" role=\"img\" title=\"point right\">:point_right:</span> Please use the following zoom link to attend the lecture: <a href=\"https://authgr.zoom.us/j/91839826029\">https://authgr.zoom.us/j/91839826029</a><br>\n<span aria-label=\"light bulb\" class=\"emoji emoji-1f4a1\" role=\"img\" title=\"light bulb\">:light_bulb:</span> Learn more about the lecture: <a href=\"http://www.i-aida.org/ai-lectures/\">http://www.i-aida.org/ai-lectures/</a></p>\n<p>The lecture is FREE!</p>\n<p>Abstract: Computers are still largely not connotated with emotional intelligence ‚Äì even more than two decades after the kick-off of the Affective Computing as the core discipline in this regard. Yet, recently significant advancement took place in the recognition of human emotion and generation of simulated emotional behaviour by computing devices increasingly lending them ‚ÄúArtificial Emotional Intelligence‚Äù. This can open up a rich selection of exciting applications to become reality such as completely changing how we interact with computing devices. In this talk, we will dive deep into the latest developments in multimodal Affective Computing from the AI perspective. This includes self-learning of neural architectures by AutoML, reinforcement learning, lifelong and self-supervised learning, ‚Äúgreen‚Äù efficient learning, federated learning, but also using emotion in learning itself. Furthermore, we will look into robustness issues such as against adversarial attacks or package loss. Beyond showing these and further recent trends and developments largely basing on deep learning techniques, the talk will end on the major needed final steps at ‚ÄúT-minus 3‚Äù to make Artificial Emotional Intelligence take-off and ‚Äúfly‚Äù in real-world applications at scale.</p>",
        "id": 227749654,
        "sender_full_name": "Anna Tahovsk√° (CLAIRE Office, CZ)",
        "timestamp": 1614250800
    },
    {
        "content": "<p>On behalf of <a href=\"https://www.vision4ai.eu/\">VISION Project</a>, I would like to invite you to one of the upcoming joint ICT48 activities under the AIDA AI Excellence Lecture Series:<br>\n<strong>Lecture by Prof. Andreas Geiger: Towards Robust End-to-End Driving</strong></p>\n<p><span aria-label=\"calendar\" class=\"emoji emoji-1f4c5\" role=\"img\" title=\"calendar\">:calendar:</span> Tuesday <strong>Mar 23rd, 2021, 17:00 ‚Äì 18:00 CET</strong><br>\n<span aria-label=\"point right\" class=\"emoji emoji-1f449\" role=\"img\" title=\"point right\">:point_right:</span> Please use the following zoom link to attend the lecture: <a href=\"https://authgr.zoom.us/j/92425987539\">https://authgr.zoom.us/j/92425987539</a><br>\n<span aria-label=\"light bulb\" class=\"emoji emoji-1f4a1\" role=\"img\" title=\"light bulb\">:light_bulb:</span> Learn more about the lecture: <a href=\"http://www.i-aida.org/ai-lectures/#event\">http://www.i-aida.org/ai-lectures/#event</a></p>\n<p>The lecture is FREE!</p>\n<p>Abstract: I will present several recent results of my group on learning robust driving policies that have advanced the state-of-the-art in the CARLA self-driving simulation environment. To generalize across diverse conditions, humans leverage multiple types of situation-specific reasoning and learning strategies. Motivated by this observation, I will first present a framework for learning situational driving policies that effectively captures reasoning under varying types of scenarios and leads to 98% success rate on the CARLA self-driving benchmark as well as state-of-the-art performance on a novel generalization benchmark. Next, I will discuss the problem of covariate shift in imitation learning. I will demonstrate that existing data aggregation techniques for addressing this problem have poor generalization performance, and present a novel approach with empirically better generalization performance. Finally, I will talk about the importance of intermediate representations and attention for learning robust self-driving models.</p>",
        "id": 229810724,
        "sender_full_name": "Anna Tahovsk√° (CLAIRE Office, CZ)",
        "timestamp": 1615450567
    },
    {
        "content": "<p>On behalf of <a href=\"https://www.vision4ai.eu/\">VISION Project</a>, I would like to invite you to one of the upcoming joint ICT48 activities under the AIDA AI Excellence Lecture Series:</p>\n<p><strong>Lecture by Prof. LP Morency: ‚ÄúMultimodal AI: Understanding Human Behaviors‚Äù</strong></p>\n<p><span aria-label=\"calendar\" class=\"emoji emoji-1f4c5\" role=\"img\" title=\"calendar\">:calendar:</span> Tuesday <strong>Apr 6th, 2021 17:00 ‚Äì 18:00 CET</strong><br>\n<span aria-label=\"point right\" class=\"emoji emoji-1f449\" role=\"img\" title=\"point right\">:point_right:</span> Please use the following zoom link to attend the lecture: <a href=\"https://authgr.zoom.us/j/94136791572\">https://authgr.zoom.us/j/94136791572</a><br>\n<span aria-label=\"light bulb\" class=\"emoji emoji-1f4a1\" role=\"img\" title=\"light bulb\">:light_bulb:</span> Learn more about the lecture: <a href=\"http://www.i-aida.org/ai-lectures/#event\">http://www.i-aida.org/ai-lectures/#event</a></p>\n<p>The lecture is FREE!</p>\n<p>Abstract: Human face-to-face communication is a little like a dance, in that participants continuously adjust their behaviors based on verbal and nonverbal cues from the social context. Today‚Äôs computers and interactive devices are still lacking many of these human-like abilities to hold fluid and natural interactions. Leveraging recent advances in machine learning, audio-visual signal processing and computational linguistic, my research focuses on creating computational technologies able to analyze, recognize and predict human subtle communicative behaviors in social context. Central to this research effort is the introduction of new probabilistic models able to learn the temporal and fine-grained latent dependencies across behaviors, modalities and interlocutors. In this talk, I will present some of our recent achievements in multimodal machine learning, addressing five core challenges: representation, alignment, fusion, translation and co-learning.</p>",
        "id": 232231842,
        "sender_full_name": "Anna Tahovsk√° (CLAIRE Office, CZ)",
        "timestamp": 1617005145
    },
    {
        "content": "<p>On behalf of <a href=\"https://www.vision4ai.eu/\">VISION Project</a>, I would like to invite you to one of the upcoming joint ICT48 activities under the AIDA AI Excellence Lecture Series:</p>\n<p><strong>Lecture by Prof. Nicu Sebe: ‚ÄúImage and Video Generation: A deep Learning Approach‚Äù</strong> </p>\n<p><span aria-label=\"calendar\" class=\"emoji emoji-1f4c5\" role=\"img\" title=\"calendar\">:calendar:</span> Tuesday <strong>Apr 20th, 2021, 17:00 ‚Äì 18:00 CET</strong><br>\n<span aria-label=\"point right\" class=\"emoji emoji-1f449\" role=\"img\" title=\"point right\">:point_right:</span> Please use the following zoom link to attend the lecture: <a href=\"https://authgr.zoom.us/j/99459774837\">https://authgr.zoom.us/j/99459774837</a><br>\n<span aria-label=\"light bulb\" class=\"emoji emoji-1f4a1\" role=\"img\" title=\"light bulb\">:light_bulb:</span> Learn more about the lecture: <a href=\"http://www.i-aida.org/ai-lectures/#event\">http://www.i-aida.org/ai-lectures/#event</a></p>\n<p>Passcode: 649253<br>\nThe lecture is FREE!</p>\n<p>Abstract: <br>\nVideo generation consists of generating a video sequence so that an object in a source image is animated according to some external information (a conditioning label or the motion of a driving video). In this talk I will present some of our recent achievements adressing these specific aspects: 1) generating facial expressions, e.g., smiles that are different from each other (e.g., spontaneous, tense, etc.) using diversity as the driving force. 2) generating videos without using any annotation or prior information about the specific object to animate.</p>\n<p>Once trained on a set of videos depicting objects of the same category (e.g. faces, human bodies), our method can be applied to any object of this class. To achieve this, we decouple appearance and motion information using a self-supervised formulation. To support complex motions, we use a representation consisting of a set of learned keypoints along with their local affine transformations. A generator network models occlusions arising during target motions and combines the appearance extracted from the source image and the motion derived from the driving video. Our solutions score best on diverse benchmarks and on a variety of object categories.</p>",
        "id": 234817736,
        "sender_full_name": "Anna Tahovsk√° (CLAIRE Office, CZ)",
        "timestamp": 1618563197
    },
    {
        "content": "<p>On behalf of <a href=\"https://www.vision4ai.eu/\">VISION Project</a>, I would like to invite you to one of the upcoming joint ICT48 activities under the AIDA AI Excellence Lecture Series:</p>\n<p><strong>Lecture by Prof. Anibal Ollero: ‚ÄúToward efficient and safe intelligent aerial robotics  and aerial manipulation‚Äù</strong></p>\n<p><span aria-label=\"calendar\" class=\"emoji emoji-1f4c5\" role=\"img\" title=\"calendar\">:calendar:</span> Tuesday <strong>May 4th, 2021, 17:00 ‚Äì 18:00 CET</strong><br>\n<span aria-label=\"point right\" class=\"emoji emoji-1f449\" role=\"img\" title=\"point right\">:point_right:</span> Please use the following zoom link to attend the lecture: <a href=\"https://authgr.zoom.us/j/91092951412\">https://authgr.zoom.us/j/91092951412</a><br>\n<span aria-label=\"light bulb\" class=\"emoji emoji-1f4a1\" role=\"img\" title=\"light bulb\">:light_bulb:</span> Learn more about the lecture: <a href=\"http://www.i-aida.org/ai-lectures/#event\">http://www.i-aida.org/ai-lectures/#event</a></p>\n<p>Passcode: 148148<br>\nThe lecture is FREE!</p>\n<p>Abstract:<br>\nIn this lecture I will review problems related to the development of autonomous aerial robots able to have long range- endurance flights, but also to interact physically with persons and objects in the environment. I will also consider aerial manipulation, including manipulation while flying but also after perching. In the lecture I will present results of the ERC Advanced Grant GRIFFIN and also of the H2020 projects AERIAL-CORE and HYFLIERS.</p>",
        "id": 236291508,
        "sender_full_name": "Anna Tahovsk√° (CLAIRE Office, CZ)",
        "timestamp": 1619506781
    },
    {
        "content": "<p>On behalf of <a href=\"https://www.vision4ai.eu/\">VISION Project</a>, I would like to invite you to one of the upcoming joint ICT48 activities under the <a href=\"http://www.i-aida.org/\">AIDA</a> AI Excellence Lecture Series:</p>\n<p><strong>Lecture by Prof. John Shawe-Taylor: ‚ÄúAn Introduction to PAC-Bayesian Analysis‚Äù</strong></p>\n<p><span aria-label=\"calendar\" class=\"emoji emoji-1f4c5\" role=\"img\" title=\"calendar\">:calendar:</span> Tuesday <strong>May 18th, 2021, 17:00 ‚Äì 18:00 CET</strong><br>\n<span aria-label=\"point right\" class=\"emoji emoji-1f449\" role=\"img\" title=\"point right\">:point_right:</span> Please use the following zoom link to attend the lecture: <a href=\"https://authgr.zoom.us/j/98708421450\">https://authgr.zoom.us/j/98708421450</a><br>\n<span aria-label=\"light bulb\" class=\"emoji emoji-1f4a1\" role=\"img\" title=\"light bulb\">:light_bulb:</span> Learn more about the lecture: <a href=\"http://www.i-aida.org/ai-lectures/#event\">http://www.i-aida.org/ai-lectures/#event</a></p>\n<p>Passcode: 148148<br>\nThe lecture is FREE!</p>\n<p>Abstract:<br>\nThe lecture will introduce the key ideas of statistical learning theory and its role in analysing learning systems. We describe the PAC-Bayesian approach to bounding generalisation and demonstrate its application to support vector machines. After discussing extensions of the approach, we will give examples of more recent applications to deep learning with promising results.</p>",
        "id": 238598587,
        "sender_full_name": "Anna Tahovsk√° (CLAIRE Office, CZ)",
        "timestamp": 1620897227
    },
    {
        "content": "<p>On behalf of <a href=\"https://www.vision4ai.eu/\">VISION Project</a>, I would like to invite you to one of the upcoming joint ICT48 activities under the <a href=\"http://www.i-aida.org/\">AIDA</a> AI Excellence Lecture Series:</p>\n<p><strong>Lecture by Prof. Isabelle Bloch: ‚ÄúHybrid AI for knowledge representation and model-based medical image understanding‚Äù</strong></p>\n<p><span aria-label=\"calendar\" class=\"emoji emoji-1f4c5\" role=\"img\" title=\"calendar\">:calendar:</span> Tuesday <strong>June 8th, 2021, 17:00 ‚Äì 18:00 CET</strong><br>\n<span aria-label=\"point right\" class=\"emoji emoji-1f449\" role=\"img\" title=\"point right\">:point_right:</span> Please use the following zoom link to attend the lecture: <a href=\"https://authgr.zoom.us/j/94165524933\">https://authgr.zoom.us/j/94165524933</a><br>\n<span aria-label=\"light bulb\" class=\"emoji emoji-1f4a1\" role=\"img\" title=\"light bulb\">:light_bulb:</span> Learn more about the lecture: <a href=\"http://www.i-aida.org/ai-lectures/#event\">http://www.i-aida.org/ai-lectures/#event</a></p>\n<p>Passcode: 148148<br>\nThe lecture is FREE!</p>\n<p>Abstract:<br>\nImage understanding benefits from the modeling of knowledge about both the scene observed and the objects it contains as well as their relationships. We show in this context the contribution of hybrid artificial intelligence, combining different types of formalisms and methods, and combining knowledge with data. Knowledge representation may rely on symbolic and qualitative approaches, as well as semi-qualitative ones to account for their imprecision or vagueness. Structural information can be modeled in several formalisms, such as graphs, ontologies, logical knowledge bases, or neural networks, on which reasoning will be based. The problem of image understanding is then expressed as a problem of spatial reasoning. These approaches will be illustrated with examples in medical imaging, illustrating the usefulness of combining several approaches.</p>",
        "id": 239380238,
        "sender_full_name": "Anna Tahovsk√° (CLAIRE Office, CZ)",
        "timestamp": 1621410176
    },
    {
        "content": "<p><a href=\"https://twitter.com/mincienciasco/status/1395393880464400392?s=24\">https://twitter.com/mincienciasco/status/1395393880464400392?s=24</a></p>\n<div class=\"inline-preview-twitter\"><div class=\"twitter-tweet\"><a href=\"https://twitter.com/mincienciasco/status/1395393880464400392?s=24\"><img class=\"twitter-avatar\" src=\"https://uploads.zulipusercontent.net/23a871740b0e2e328324c91d1ac200d8e4db0934/68747470733a2f2f7062732e7477696d672e636f6d2f70726f66696c655f696d616765732f313339343238323138323130313834383036382f71424645755435735f6e6f726d616c2e706e67\"></a><p>ü§©¬°Cont√©mosles a todos que esta es la Semana de la Afrocolombianidad!\n\n<span aria-label=\"selfie\" class=\"emoji emoji-1f933\" role=\"img\" title=\"selfie\">:selfie:</span>Personaliza tu foto de perfil en 1 paso, haciendo clic en la red donde quieres poner nuestro marco<span aria-label=\"blush\" class=\"emoji emoji-1f60a\" role=\"img\" title=\"blush\">:blush:</span>:\n\n<span aria-label=\"blue circle\" class=\"emoji emoji-1f535\" role=\"img\" title=\"blue circle\">:blue_circle:</span>Twitter: <a href=\"https://t.co/QA1eTx7QNk\">https://bit.ly/2S3rEB3</a>\nüü¶Facebook: <a href=\"https://t.co/PNHCxs1gpH\">https://bit.ly/3tPKzwk</a>\n\n#AfrocolombianidadYCiencia <a href=\"https://t.co/7rgpaDKUiU\">https://twitter.com/MincienciasCo/status/1395393880464400392/photo/1</a></p><span>- Minciencias Colombia (@MincienciasCo)</span><div class=\"twitter-image\"><a href=\"https://t.co/7rgpaDKUiU\"><img src=\"https://uploads.zulipusercontent.net/b7f2664494923a703d53e739cdff79f47af1b2f2/68747470733a2f2f7062732e7477696d672e636f6d2f6d656469612f45316f7766666f5830414d7566384b2e6a70673a736d616c6c\"></a></div></div></div>",
        "id": 239608727,
        "sender_full_name": "Sonia Castro",
        "timestamp": 1621523589
    }
]