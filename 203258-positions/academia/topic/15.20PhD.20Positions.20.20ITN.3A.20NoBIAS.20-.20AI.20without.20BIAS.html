---
layout: archive
title: Zulip Chat Archive
permalink: /stream/203258-positions/academia/topic/15.20PhD.20Positions.20.20ITN.3A.20NoBIAS.20-.20AI.20without.20BIAS.html
---

<h2>Stream: <a href="https://claire4ai.github.io/archive/stream/203258-positions/academia/index.html">positions/academia</a></h2>
<h3>Topic: <a href="https://claire4ai.github.io/archive/stream/203258-positions/academia/topic/15.20PhD.20Positions.20.20ITN.3A.20NoBIAS.20-.20AI.20without.20BIAS.html">15 PhD Positions  ITN: NoBIAS - AI without BIAS</a></h3>

<hr>

<base href="https://claire.zulipchat.com">

<head><link href="/style.css" rel="stylesheet"></head>

{% raw %}

<a name="188032515"></a>
<h4><a href="https://claire.zulipchat.com#narrow/stream/203258-positions/academia/topic/15%20PhD%20Positions%20%20ITN%3A%20NoBIAS%20-%20AI%20without%20BIAS/near/188032515" class="zl"><img src="https://claire4ai.github.io/archive/assets/img/zulip2.png" alt="view this post on Zulip"></a> Ricardo Chavarriaga (CLAIRE Office Switzerland) <a href="https://claire4ai.github.io/archive/stream/203258-positions/academia/topic/15.20PhD.20Positions.20.20ITN.3A.20NoBIAS.20-.20AI.20without.20BIAS.html#188032515">(Feb 12 2020 at 17:26)</a>:</h4>
<p><a href="https://nobias-project.eu/index.php/early-stage-researcher-phd-projects/" target="_blank" title="https://nobias-project.eu/index.php/early-stage-researcher-phd-projects/">https://nobias-project.eu/index.php/early-stage-researcher-phd-projects/</a></p>
<p>15 PhD positions are offered within NoBIAS- Artificial Intelligence without Bias, a project funded by the European Unionâ€™s Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie Grant Agreement No. 860630.</p>
<p>NoBIAS aims at developing novel methods for AI-based decision making without bias by taking into account ethical and legal considerations in the design of technical solutions. The core objectives of NoBIAS are to understand legal, social and technical challenges of bias in AI-decision making, to counter them by developing fairness-aware algorithms to automatically explain AI results, and to document the overall process for data provenance and transparency.</p>
<p>Locations of the positions: Germany, Greece, Italy, United Kingdom, Belgium.</p>
<p>GESIS-1: Socio-technical networks of bias generation (GESIS-Leibniz Institute for the Social Sciences, Cologne, Germany)<br>
KULEUVEN-1: Discovering biased representations of people (KU Leuven, Belgium)<br>
CERTH-1: Discovering and quantifying bias (CERTH, Thessaloniki, Greece)<br>
UNIPI-1: Causality analysis of data for domain-specific bias understanding (University of Pisa, Italy)<br>
L3S-1: Documenting bias in data through ontologies (Leibniz University Hannover, Germany)<br>
OU-1: Data enrichment for mitigating bias (Open University, United Kingdom)<br>
CERTH-2: Bias mitigation in classification methods (CERTH, Thessaloniki, Greece)<br>
GESIS-2: Bias mitigation in ranking methods (GESIS, Germany).<br>
IRI-1: Legal issues of mitigating bias at a national and EU level (Leibniz University Hannover, Germany)<br>
ECS-2: Explaining prediction models by argumentation (University of Southampton, United Kingdom)<br>
SCHUFA-1: Explaining white-model predictions (SCHUFA, Germany)<br>
ECS-1: Collective elicitation of bias coming from inaccessible algorithms (University of Southampton, United Kingdom)<br>
UNIPI-2: Declarative explanation of black-box decisions (University of Pisa, Italy)<br>
LS-1: Combining a technical, legal, and ethical assessment of the rights to information and explanation (University of Southampton, United Kingdom)<br>
L3S-2: Time-dependent monitoring and mitigation for bias (Leibniz University Hannover, Germany)</p>



{% endraw %}

<hr><p>Last updated: Mar 09 2021 at 03:53 UTC</p>